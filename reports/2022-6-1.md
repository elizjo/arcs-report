# Activities

### Pomona's Server
+ followed parts 1 to 6 on [Connecting to the Pomona College HPC](https://cs.pomona.edu/classes/cs152/pages/pomona-hpc.mdeep.html#usefulandrequiredenvironmentvariablesandaliases)
+ consulted vscode documentation [Remote Development using SSH](https://code.visualstudio.com/docs/remote/ssh)
+ realized it's more efficient to edit ssh config file to permamently save configurations to connect to server via vscode 
<img width="250" alt="image" src="https://user-images.githubusercontent.com/78676977/171686093-5b8cbdbf-fb4d-41ff-8a9d-b654b3e79990.png">


### Terminal 
+ created alias terms to easily enable commands 
<img width="537" alt="alias_example" src="https://user-images.githubusercontent.com/78676977/171508111-e8af89ef-636a-4f89-9ed9-3dabc728bff1.png">
+ reviewed & learned basic command terms (cd, pwd, ls, cd ..)
  + partially typing command + tab allows for autocomplete


### fastai 
+ completed Lesson 1/ Beginner Vision Tutorial
  + was famliar w/ single label classification, but learned about and implemented segmentation & multi-label classification
  + built multiple pre-trained models using resnet34
 
+ created 2 neural net models with two ARCS Datasets (uniform-small & corrected-wander-full)
  + fastai models use path objects, wheras the prior nn models I've made used raw data from csv files & pandas to manipulate data
  + was not familiar with Path objects, but consulted [Python documentation](https://docs.python.org/3/library/pathlib.html#pathlib.Path)
  + did not know how to create function to label images based on its filename, consulted past student work 

### Photogrammetry
+ learned the workflow on taking images -> reality capture -> unreal engine watching [RealityCapture to UE5 - Workflow Tutorial - YouTube](https://www.youtube.com/watch?app=desktop&v=WrCOhes1Zgg)


# Issues
+ still manually connecting to juypter notebook every session 
+ was confused on when I was working on my local machine v. Pomona server
  + resolved through Prof. Clark's explanations + setup on vscode
+ vision_learner, plot_interp_matrix on fastai were not in less updated version of fastai on our virtual enviroment
  + resolved using older name (cnn_learner) and found alternative function on github docs to fix plot_interp_matrix
+ realized I needed to be granted permission to access datasets to execute certain functions (lr_find()) from fastai


# Plans
+ 
+ further understanding on conceptual aspects of nn's

# Article Summaries

[Visualizing and Understanding Convolutional Networks](https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_53.pdf)
I was not sure why pre-trained models worked and outperformed models built by scratch. To visualize the feature activity of convolutional layers, Zeiler and Fergus attached a novel implementation of deconvolutional layer subsequently after each convolutional layer in their observed model. Activity of the convolutional layer was reconstructed by unpooling, rectifying, and filering the result of applying the decovnet model to create a feature map. The generated feature map from the deconvolutional network looked extremely similar to images in the data and additionally showed which parts of the input image were discriminative. To distinguish if decision making was based on the location of the object within the image versus surrounding context, authors occuluded parts of the image. This resulted in a significant decrease in the probability of predicting the correct label, validating that classifying behavior of the model was reliant on the image features, not surroundings. Zeiler and Fergus concluded that features convolutational neural nets use are deliberate, interpretable patterns and demonstrate large capacity to be generalized to other image datasets.

[Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation](https://cs.pomona.edu/~ajc/pdf/Chang.2021.SSCI.Architectures.pdf)

576 standard classification models were created from 24 architectures, 3 artifical datasets, 2 pre-training options, and 4 replicates. Models from scratch and transfer learning via pre-trained models were built. Artifical data collection involved using three approaches in their simulation: handmade, uniform, and wandering. The handmade dataset, images captured from manually guiding the robot, produced worse-performing models due to the lack of noise. Too few examples of deviations from the ideal path were present in the dataset. Additionanlly, pretrained models were overfit to the uniform dataset but not the wandering dataset and from scratch models worked best with the wandering dataset. To prevent robot navigation from becoming stuck in a loop, authors employed two methods to maintain a memory of past input/ actions to be used in future classification behavior. Accessible and well-performing models were achieved using Image+Command hybrid models, in which hybrid models explicity include the previous output of the model as input for the next step. Authors note that training models from scratch may lead to better results, and propose future work: use on real-world device, methods to decrease reality gap, and hyperperameter tuning. 
