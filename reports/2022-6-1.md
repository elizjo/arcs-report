# Activities

### Pomona's Server
+ followed parts 1 to 6 on [Connecting to the Pomona College HPC](https://cs.pomona.edu/classes/cs152/pages/pomona-hpc.mdeep.html#usefulandrequiredenvironmentvariablesandaliases)
+ consulted vscode documentation [Remote Development using SSH](https://code.visualstudio.com/docs/remote/ssh)
+ realized it's more efficient to edit ssh config file to permamently save configurations to connect to server via vscode 
  <img width="392" alt="image of ssh config file" src="https://user-images.githubusercontent.com/78676977/171508606-e0c17c4d-2c6f-47bc-8591-15a3256fade8.png">


### Terminal 
+ created alias terms to easily enable commands 
<img width="537" alt="alias_example" src="https://user-images.githubusercontent.com/78676977/171508111-e8af89ef-636a-4f89-9ed9-3dabc728bff1.png">

+ reviewed & learned basic command terms (cd, pwd, ls, cd ..)
+ partially typing command + tab allows for autocomplete


### fastai 
+ completed Lesson 1/ Beginner Vision Tutorial
  + was famliar w/ single label classification, but learned about and implemented segmentation & multi-label classification
  + built multiple pre-trained models using resnet34
 
+ created 2 neural net models with two ARCS Datasets (uniform-small & corrected-wander-full)
  + fastai models use path objects, wheras the prior nn models I've made used raw data from csv files & pandas to manipulate data
  + was not familiar with Path objects, but consulted [Python documentation](https://docs.python.org/3/library/pathlib.html#pathlib.Path)
  + did not know how to create function to label images based on its filename, consulted past student work 

### Photogrammetry
+ learned the workflow on taking images -> reality capture -> unreal engine watching [RealityCapture to UE5 - Workflow Tutorial - YouTube](https://www.youtube.com/watch?app=desktop&v=WrCOhes1Zgg)


# Issues
+ still manually connecting to juypter notebook every session 
+ was confused on when I was working on my local machine v. Pomona server
  + resolved through Prof. Clark's explanations + setup on vscode
+ vision_learner, plot_interp_matrix on fastai were not in less updated version of fastai on our virtual enviroment
  + resolved using older name (cnn_learner) and found alternative function on github docs to fix plot_interp_matrix
+ realized I needed to be granted permission to access datasets to execute certain functions (lr_find()) from fastai


# Plans
+ 
+ further understanding on conceptual aspects of nn's

# Article Summaries

[Visualizing and Understanding Convolutional Networks](https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_53.pdf)
I was not sure why pre-trained models worked and outperformed models built by scratch. To visualize the feature activity of convolutional layers, Zeiler and Fergus attached a novel implementation of deconvolutional layer subsequently after each convolutional layer in their observed model. Activity of the convolutional layer was reconstructed by unpooling, rectifying, and filering the result of applying the decovnet model to create a feature map. The generated feature map from the deconvolutional network looked extremely similar to images in the data and additionally showed which parts of the input image were discriminative. To distinguish if decision making was based on the location of the object within the image versus surrounding context, authors occuluded parts of the image. This resulted in a significant decrease in the probability of predicting the correct label, validating that classifying behavior of the model was reliant on the image features, not surroundings. Zeiler and Fergus concluded that features convolutational neural nets use are deliberate, interpretable patterns and demonstrate large capacity to be generalized to other image datasets.

[Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation](https://cs.pomona.edu/~ajc/pdf/Chang.2021.SSCI.Architectures.pdf)

artifical data collection using three approaches in the simulation: handmade, uniform, and wandering. The handmade dataset produced worse-performing models due to the lack of noise: too few examples of deviations from the ideal path were present in the dataset.  from scratch v. transfer learning via pre-trained models. Best results were achieved using Image+Command hybrid models.
